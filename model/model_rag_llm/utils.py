import os
import re
import json
import time
import psycopg2
from sqlalchemy import create_engine
from dotenv import load_dotenv
from unsloth import FastModel
from unsloth import FastLanguageModel
from unsloth.chat_templates import get_chat_template

# í™˜ê²½ ì„¤ì •
load_dotenv()
vdb_user = os.getenv("VDB_USER")
vdb_password = os.getenv("VDB_PASSWORD")
vdb_host = os.getenv("VDB_HOST")
vdb_port = int(os.getenv("VDB_PORT", "5432"))
vdb_name = os.getenv("VDB_NAME")
vdb_engine = create_engine(f"postgresql+psycopg2://{vdb_user}:{vdb_password}@{vdb_host}:{vdb_port}/{vdb_name}")

# ëª¨ë¸ ê²½ë¡œ: /Users/hanseungheon/Desktop/Google_ML_NIPA/Integration/model_server/model/model_rag_llm/gemma-3-reviews-model
# ëª¨ë¸ ê²½ë¡œ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
MODEL_PATH = "/Users/hanseungheon/Desktop/Google_ML_NIPA/Integration/model_server/model/model_rag_llm/gemma-3-reviews-model"

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì €ì¥í•  ì „ì—­ ë³€ìˆ˜
MODEL = None
TOKENIZER = None

def vector_db_conn():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•¨ìˆ˜"""
    try:
        conn = psycopg2.connect(
            host=vdb_host,
            port=vdb_port,
            dbname=vdb_name,
            user=vdb_user,
            password=vdb_password
        )
        return conn
    except Exception as e:
        print(f"ğŸš« [ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜] {str(e)}")
        raise

def load_finetuned_model(model_path):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜"""
    print(f"ğŸ“‚ [ëª¨ë¸ ë¡œë”©] íŒŒì¸íŠœë‹ëœ Gemma-3 ëª¨ë¸ ë¡œë”© ì¤‘... (ê²½ë¡œ: {model_path})")
    try:
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_path,
            max_seq_length=2048,
            load_in_4bit=True,
            load_in_8bit=False,
        )
        # ì±„íŒ… í…œí”Œë¦¿ ì„¤ì •
        tokenizer = get_chat_template(
            tokenizer,
            chat_template="gemma-3",
        )
        print("âœ… [ëª¨ë¸ ë¡œë”©] ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        return model, tokenizer
    except Exception as e:
        print(f"ğŸš« [ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜] {str(e)}")
        raise

def initialize_model(model_path=None, force_reload=False):
    """ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜ - ìºì‹± ì§€ì›"""
    global MODEL, TOKENIZER
    
    # ëª¨ë¸ ê²½ë¡œê°€ ëª…ì‹œì ìœ¼ë¡œ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©
    if model_path is None:
        model_path = MODEL_PATH
        
    if MODEL is None or force_reload:
        MODEL, TOKENIZER = load_finetuned_model(model_path)

def clean_review_line(review_line):
    """ë¦¬ë·° ë¼ì¸ì—ì„œ ì•ì— ë¶™ì€ '-' ê¸°í˜¸ ë“±ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜"""
    if review_line is None:
        return None
    # ë¦¬ë·° ë¼ì¸ì˜ ì•ë¶€ë¶„ì— ìˆëŠ” '-' ê¸°í˜¸ì™€ ê³µë°± ì œê±°
    return re.sub(r'^-\s*', '', review_line.strip())

def process_reviews(reviews):
    """ë¦¬ë·° ì „ì²˜ë¦¬ ë° í˜•ì‹í™” í•¨ìˆ˜"""
    # ë¹ˆ ë¦¬ë·°ë‚˜ ì˜ë¯¸ ì—†ëŠ” ë¦¬ë·° í•„í„°ë§
    filtered_reviews = [r for r in reviews if r and r.strip() and r != "(ë¦¬ë·° ì—†ìŒ)"]
    
    # ë¦¬ë·°ê°€ ë§ì„ ê²½ìš° ê°€ì¥ ì •ë³´ëŸ‰ì´ ë§ì€ ë¦¬ë·°ë§Œ ì„ íƒ (ê¸´ ë¦¬ë·° ìš°ì„ )
    if len(filtered_reviews) > 7:
        filtered_reviews = sorted(filtered_reviews, key=len, reverse=True)[:7]
    
    # ë¦¬ë·°ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì¶”ê°€
    if not filtered_reviews:
        filtered_reviews = ["(ë¦¬ë·° ì—†ìŒ)"]
    
    return filtered_reviews

def parse_llm_response(response_text):
    """LLM ì‘ë‹µì„ ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    if not response_text or not response_text.strip():
        return {"review_1": None, "review_2": None, "review_3": None}
    
    # ë¦¬ë·° ì¤„ ì¶”ì¶œ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)
    lines = response_text.split('\n')
    
    # ëª¨ë“  ì¤„ì—ì„œ '- ' ì ‘ë‘ì–´ ì œê±° ë° ë¹ˆ ì¤„ ì œê±°
    lines = [re.sub(r'^-\s*', '', line.strip()) for line in lines if line.strip()]
    
    # ë‹¨ì¼ ì¤„ì´ ì—¬ëŸ¬ ë¬¸ì¥ì„ í¬í•¨í•˜ëŠ” ê²½ìš° ì²˜ë¦¬
    if len(lines) == 1:
        sentences = re.split(r'(?<=[.!?~])\s+', lines[0].strip())
        lines = [s.strip() for s in sentences if s.strip()]
    
    # ì •í™•íˆ 3ê°œì˜ ë¦¬ë·° ë³´ì¥
    while len(lines) < 3:
        lines.append(None)
    
    # ìµœëŒ€ 3ê°œì˜ ë¦¬ë·°ë§Œ ì‚¬ìš©
    lines = lines[:3]
    
    return {
        "review_1": lines[0],
        "review_2": lines[1],
        "review_3": lines[2]
    }

def get_review_prompt_template():
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬"""
    return """
    ## Persona
    ë‹¹ì‹ ì€ 'ë§¤ì¥ëª…', 'ë§¤ì¥ë¦¬ë·°'ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§¤ì¥ì— ëŒ€í•œ ì •ë³´ë¥¼ 3ì¤„(100ì ì´ë‚´)ë¡œ ë‹µë³€í•˜ëŠ” 3ëª…ì˜ ë¦¬ë·°ì–´ì…ë‹ˆë‹¤.

    ## Instruction
    - ëª¨ë“  ë¬¸ì¥ì€ 'í•œêµ­ì–´'ë¡œ ì‘ì„±í•˜ê³ , 3ì¤„(100ì ì´ë‚´)ì˜ ì™„ê²°í˜• ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
    - ë°˜ë“œì‹œ 'ë°©ë¬¸ì'ì˜ ì‹œì ì—ì„œ ì§ì ‘ ê²½í—˜í•œ ê²ƒì²˜ëŸ¼ ì‘ì„±í•˜ì„¸ìš”.
    - ê° ì¤„ì€ ë§ˆì¹˜ ì‹¤ì œ ì‚¬ëŒì´ ë§í•˜ë“¯ ìì—°ìŠ¤ëŸ½ê³  ìƒë™ê° ìˆëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
    - ì˜¤ë¡œì§€ 'ë§¤ì¥ë¦¬ë·°'ì— ìˆëŠ” ì •ë³´ë§Œì„ í™œìš©í•´ì„œ ë‹µë³€í•©ë‹ˆë‹¤.
    - ë‹µë³€ì€ ì˜¤ë¡œì§€ ë§¤ì¥ ë¦¬ë·° 3ì¤„ë§Œ ì œê³µí•´ ì£¼ì„¸ìš”. ë§¤ì¥ ë¦¬ë·°ë¥¼ ì œì™¸í•œ ì´ì™¸ì˜ ë‹µë³€ì€ ì œê±°í•©ë‹ˆë‹¤.

    ## Information
    - **ë§¤ì¥ëª…**: {store_name}
    - **ë§¤ì¥ë¦¬ë·°**:
    {formatted_reviews}

    ## Example
    USER: ë§¤ì¥ ì •ë³´ë¥¼ 3ì¤„ë¡œ ì œê³µí•´ ì£¼ì„¸ìš”
    ASSISTANT: 
    - ë„ˆë¬´ ì¢‹ì•˜ì–´ìš”! ë‹¤ìŒì— ì¹œêµ¬ë“¤ì´ë‘ í•œ ë²ˆ ë” ì˜¤ë ¤êµ¬ìš”!
    - ë‚ ì”¨ ì¢‹ì€ ë‚ ì— ì²˜ìŒ ê°€ë´¤ëŠ”ë°, ëŒ€ë°• ì¢‹ì•˜ì–´ìš”~!
    - ë§¤ì¥ì´ ê¹”ë”í•´ì„œ ì¢‹ê³ , ì‚¬ì¥ë‹˜ì´ ì—„ì²­ ì¹œì ˆí•´ìš”~
    """

def generate_with_finetuned_model(system_prompt, user_prompt, max_retries=2):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ì¶”ë¡  ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ - ì¬ì‹œë„ ë¡œì§ ì¶”ê°€"""
    # ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if MODEL is None or TOKENIZER is None:
        raise ValueError("ëª¨ë¸ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
    
    retries = 0
    while retries <= max_retries:
        try:
            # ë©”ì‹œì§€ í¬ë§·íŒ…
            messages = [{
                "role": "user",
                "content": system_prompt + "\nUSER: " + user_prompt
            }]
            
            # ì±— í…œí”Œë¦¿ ì ìš©
            text = TOKENIZER.apply_chat_template(
                messages,
                add_generation_prompt=True,
            )
            
            # ì¶”ë¡  ìˆ˜í–‰
            outputs = MODEL.generate(
                **TOKENIZER([text], return_tensors="pt").to("cuda"),
                max_new_tokens=300,
                temperature=0.7,
                top_p=0.95,
                top_k=50,
            )
            
            # ê²°ê³¼ ë””ì½”ë”© ë° ì¶œë ¥
            decoded_output = TOKENIZER.batch_decode(outputs)[0]
            
            # MODEL ì‘ë‹µ ë¶€ë¶„ ì¶”ì¶œ
            if "<start_of_turn>model\n" in decoded_output:
                response = decoded_output.split("<start_of_turn>model\n")[1].split("<end_of_turn>")[0]
                return response.strip()
            else:
                print(f"âš ï¸ [ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜] ì‹œë„ {retries+1}/{max_retries+1}: ì‘ë‹µ ì¶”ì¶œ ì‹¤íŒ¨")
                retries += 1
                # ë§ˆì§€ë§‰ ì‹œë„ì˜€ë‹¤ë©´ ì „ì²´ ì¶œë ¥ ë°˜í™˜
                if retries > max_retries:
                    print("âš ï¸ [ê²½ê³ ] ì‘ë‹µ ì¶”ì¶œ ì‹¤íŒ¨, ì „ì²´ ì¶œë ¥ ë°˜í™˜")
                    return decoded_output
        except Exception as e:
            print(f"âš ï¸ [ì¶”ë¡  ì˜¤ë¥˜] ì‹œë„ {retries+1}/{max_retries+1}: {str(e)}")
            retries += 1
            if retries > max_retries:
                raise

def process_single_store(keyword, store_info, conn):
    """ë‹¨ì¼ ë§¤ì¥ ì²˜ë¦¬ í•¨ìˆ˜"""
    store_id, store_name, confidence = store_info
    start_time = time.time()
    
    print(f"\nğŸ“Œ [ì§„í–‰ ì¤‘] ë§¤ì¥ ì²˜ë¦¬ ì‹œì‘ - '{store_name}' (ID: {store_id}, í‚¤ì›Œë“œ: '{keyword}', ì‹ ë¢°ë„: {confidence}%)")
    
    try:
        cur = conn.cursor()
        
        # í•´ë‹¹ store_idì— ëŒ€í•œ ë¦¬ë·° ì¡°íšŒ
        cur.execute("SELECT review_docs FROM stores WHERE store_id = %s", (store_id,))
        review_rows = cur.fetchall()
        cur.close()
        
        reviews = [row[0] for row in review_rows if row[0]]
        
        # ê°€ì ¸ì˜¨ ë¦¬ë·° ì¶œë ¥
        print("ğŸ“œ [ê°€ì ¸ì˜¨ ë¦¬ë·° ëª©ë¡]")
        if reviews:
            for i, review in enumerate(reviews[:3], 1):  # ì²« 3ê°œë§Œ ì¶œë ¥
                print(f"  {i}. {review}")
            if len(reviews) > 3:
                print(f"  ... ì™¸ {len(reviews)-3}ê°œ")
        else:
            print("  (ë¦¬ë·° ì—†ìŒ)")
        
        # ë¦¬ë·° ì²˜ë¦¬ ë° í˜•ì‹í™”
        processed_reviews = process_reviews(reviews)
        formatted_reviews = '\n'.join([f" -- {r}" for r in processed_reviews])
        print(f"ğŸ“ [í”„ë¡¬í”„íŠ¸ ìƒì„±] '{store_name}' ë¦¬ë·° í˜•ì‹í™” ì™„ë£Œ")
        
        # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_template = get_review_prompt_template()
        system_prompt = prompt_template.format(
            store_name=store_name,
            formatted_reviews=formatted_reviews
        )
        
        # ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶”ë¡ 
        user_prompt = "ë§¤ì¥ ì •ë³´ë¥¼ 3ì¤„ë¡œ ì œê³µí•´ ì£¼ì„¸ìš”"
        response_raw = generate_with_finetuned_model(system_prompt, user_prompt)
        
        # ê²°ê³¼ ì²˜ë¦¬
        review_parts = parse_llm_response(response_raw)
        
        print(f"âœ… [LLM ì‘ë‹µ ì™„ë£Œ] '{store_name}' ë¦¬ë·° ìš”ì•½ ìƒì„± ì™„ë£Œ")
        
        end_time = time.time()
        processing_time = end_time - start_time
        print(f"â±ï¸ [ì²˜ë¦¬ ì‹œê°„] {processing_time:.2f}ì´ˆ")
        
        return {
            "keyword": keyword,
            "store_id": store_id,
            "store_name": store_name,
            "confidence": confidence,
            "review_1": review_parts["review_1"],
            "review_2": review_parts["review_2"],
            "review_3": review_parts["review_3"],
            "processing_time": processing_time
        }
        
    except Exception as e:
        print(f"âš ï¸ [ë§¤ì¥ ì²˜ë¦¬ ì˜¤ë¥˜] '{store_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {
            "keyword": keyword,
            "store_id": store_id,
            "store_name": store_name,
            "confidence": confidence,
            "review_1": None,
            "review_2": None,
            "review_3": None,
            "error": str(e)
        }

def process_stores_in_batches(stt_results, batch_size=5):
    """ë§¤ì¥ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ íš¨ìœ¨ì„± í–¥ìƒ"""
    all_summaries = []
    items = list(stt_results.items())
    
    conn = vector_db_conn()  # ì—°ê²° í•œ ë²ˆë§Œ ìƒì„±
    
    for i in range(0, len(items), batch_size):
        batch = dict(items[i:i+batch_size])
        print(f"ğŸ”„ [ë°°ì¹˜ ì²˜ë¦¬] ë°°ì¹˜ {i//batch_size + 1}: {i+1}~{min(i+batch_size, len(items))}/{len(items)}")
        
        batch_results = []
        for keyword, store_info in batch.items():
            result = process_single_store(keyword, store_info, conn)
            batch_results.append(result)
        
        all_summaries.extend(batch_results)
    
    conn.close()  # ì‘ì—… ì™„ë£Œ í›„ ì—°ê²° ë‹«ê¸°
    return all_summaries

def log_performance_metrics(summaries):
    """ì„±ëŠ¥ ì§€í‘œ ë¡œê¹…"""
    if not summaries:
        return
    
    # ì˜¤ë¥˜ê°€ ì—†ëŠ” í•­ëª©ë§Œ í•„í„°ë§
    successful_items = [s for s in summaries if "error" not in s]
    
    if not successful_items:
        print("ğŸ•’ [ì„±ëŠ¥ ì§€í‘œ] ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì²˜ë¦¬ ì‹œê°„ì´ ìˆëŠ” í•­ëª©ë§Œ í•„í„°ë§
    timed_items = [s for s in successful_items if "processing_time" in s]
    
    if timed_items:
        total_time = sum(s["processing_time"] for s in timed_items)
        avg_time = total_time / len(timed_items)
        print(f"ğŸ•’ [ì„±ëŠ¥ ì§€í‘œ] ì´ ì²˜ë¦¬ëœ ë§¤ì¥ ìˆ˜: {len(successful_items)}")
        print(f"ğŸ•’ [ì„±ëŠ¥ ì§€í‘œ] ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"ğŸ•’ [ì„±ëŠ¥ ì§€í‘œ] ë§¤ì¥ë‹¹ í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_time:.2f}ì´ˆ")

def generate_store_summaries(stt_results: dict, model_path=None, use_batches=True, batch_size=5):
    """
    ë¦¬ë·° ìš”ì•½ ìƒì„± í•¨ìˆ˜
    
    Args:
        stt_results (dict): STTì—ì„œ ì „ë‹¬ë°›ì€ ë§¤ì¥ ì •ë³´ ë”•ì…”ë„ˆë¦¬
            {
                'ê²€ìƒ‰ì–´': [store_id, store_name, confidence],
                ...
            }
        model_path (str, optional): ëª¨ë¸ ê²½ë¡œ (Noneì¸ ê²½ìš° MODEL_PATH í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
        use_batches (bool): ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€
        batch_size (int): ë°°ì¹˜ í¬ê¸°
    
    Returns:
        dict: ì²˜ë¦¬ ê²°ê³¼ ë° ìƒì„±ëœ ë¦¬ë·° ìš”ì•½
    """
    # ì „ì²´ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    overall_start_time = time.time()
    
    try:
        # ëª¨ë¸ ì´ˆê¸°í™” (ì§€ì •ëœ ê²½ë¡œ ë˜ëŠ” ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
        initialize_model(model_path)
        
        print(f"\nğŸš€ [ì²˜ë¦¬ ì‹œì‘] ì´ {len(stt_results)}ê°œ ë§¤ì¥ ì²˜ë¦¬ ì‹œì‘")
        
        # ë°°ì¹˜ ì²˜ë¦¬ ë˜ëŠ” ê¸°ì¡´ ë°©ì‹ ì„ íƒ
        if use_batches:
            summaries = process_stores_in_batches(stt_results, batch_size)
        else:
            conn = vector_db_conn()
            summaries = []
            
            for keyword, store_info in stt_results.items():
                result = process_single_store(keyword, store_info, conn)
                summaries.append(result)
                
            conn.close()
            
        # ì „ì²´ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        overall_end_time = time.time()
        overall_duration = overall_end_time - overall_start_time
        
        # ì„±ëŠ¥ ì§€í‘œ ë¡œê¹…
        print(f"\nâ±ï¸ [ì „ì²´ ì„±ëŠ¥] ì´ ì†Œìš”ì‹œê°„: {overall_duration:.2f}ì´ˆ")
        log_performance_metrics(summaries)
        
        # ìµœì¢… ê²°ê³¼ ë¡œê·¸ í™•ì¸
        print("\nğŸ‰ [ì „ì²´ ì™„ë£Œ] ëª¨ë“  ë§¤ì¥ ìš”ì•½ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"\nğŸ§¾ [ìš”ì•½ ê²°ê³¼ ë°ì´í„°]:\n{json.dumps(summaries, ensure_ascii=False, indent=2)}")

        # ì˜¤ë¥˜ í•­ëª©ì´ ìˆëŠ”ì§€ í™•ì¸
        error_items = [s for s in summaries if "error" in s]
        success_count = len(summaries) - len(error_items)
        
        return {
            "status": "success",
            "code": 200,
            "message": f"ë§¤ì¥ ìš”ì•½ ì™„ë£Œ ({success_count}/{len(summaries)} ì„±ê³µ)",
            "processed": "ok",
            "total_time": overall_duration,
            "successful": success_count,
            "failed": len(error_items),
            "data": summaries
        }

    except Exception as e:
        print(f"\nâŒ [ì˜¤ë¥˜ ë°œìƒ] {str(e)}")
        return {
            "status": "error",
            "code": 500,
            "message": str(e),
            "processed": "failed",
            "data": []
        }

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”)
    model_path = "/path/to/gemma-3-reviews-model"
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    initialize_model(model_path)
    
    # STTì—ì„œ ì „ë‹¬ë°›ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì‹œ
    stt_input = {
        'ë§ˆë¦¬ìš°ë„¤': [3, 'ë§ˆë¦¬ì˜¤ë„¤', 64],
        'ì„±ë•': [22, 'ê°•ë³„ ì„±ìˆ˜', 62],
        'ê³ ê¸°ì§ˆì´': [82, 'ê³ ê¹ƒë°”', 62],
        'ë ˆì¸ ì‡ì¹™íŒ': [1, 'ë ˆì¸ ì‡ì¹˜í‚¨ ì„±ìˆ˜', 69],
        'ì„±ìˆ˜': [2, 'ì„±ìˆ˜ë…¸ë£¨', 85]
    }

    # í•¨ìˆ˜ ì‹¤í–‰
    result = generate_store_summaries(stt_input, batch_size=3)
    print(result)